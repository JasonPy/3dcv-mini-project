@String(PAMI  = {IEEE TPAMI})
@String(IJCV  = {IJCV})
@String(CVPR  = {CVPR})
@String(ICCV  = {ICCV})
@String(ECCV  = {ECCV})
@String(NIPS  = {NeurIPS})
@String(ICPR  = {ICPR})
@String(BMVC  =	{BMVC})
@String(TOG   = {ACM TOG})
@String(TIP   = {IEEE TIP})
@String(TVCG  = {IEEE TVCG})
@String(TCSVT = {IEEE TCSVT})
@String(TMM   =	{IEEE TMM})
@String(ACMMM = {ACM MM})
@String(ICME  =	{ICME})
@String(ICASSP=	{ICASSP})
@String(ICIP  = {ICIP})
@String(ACCV  = {ACCV})
@String(ICLR  = {ICLR})
@String(IJCAI = {IJCAI})
@String(PR = {PR})
@String(AAAI = {AAAI})
@String(CVPRW= {CVPRW})
@String(CSVT = {IEEE TCSVT})



@String(PAMI = {IEEE Trans. Pattern Anal. Mach. Intell.})
@String(IJCV = {Int. J. Comput. Vis.})
@String(CVPR= {IEEE Conf. Comput. Vis. Pattern Recog.})
@String(ICCV= {Int. Conf. Comput. Vis.})
@String(ECCV= {Eur. Conf. Comput. Vis.})
@String(NIPS= {Adv. Neural Inform. Process. Syst.})
@String(ICPR = {Int. Conf. Pattern Recog.})
@String(BMVC= {Brit. Mach. Vis. Conf.})
@String(TOG= {ACM Trans. Graph.})
@String(TIP  = {IEEE Trans. Image Process.})
@String(TVCG  = {IEEE Trans. Vis. Comput. Graph.})
@String(TMM  = {IEEE Trans. Multimedia})
@String(ACMMM= {ACM Int. Conf. Multimedia})
@String(ICME = {Int. Conf. Multimedia and Expo})
@String(ICASSP=	{ICASSP})
@String(ICIP = {IEEE Int. Conf. Image Process.})
@String(ACCV  = {ACCV})
@String(ICLR = {Int. Conf. Learn. Represent.})
@String(IJCAI = {IJCAI})
@String(PR   = {Pattern Recognition})
@String(AAAI = {AAAI})
@String(CVPRW= {IEEE Conf. Comput. Vis. Pattern Recog. Worksh.})
@String(CSVT = {IEEE Trans. Circuit Syst. Video Technol.})

@String(SPL	= {IEEE Sign. Process. Letters})
@String(VR   = {Vis. Res.})
@String(JOV	 = {J. Vis.})
@String(TVC  = {The Vis. Comput.})
@String(JCST  = {J. Comput. Sci. Tech.})
@String(CGF  = {Comput. Graph. Forum})
@String(CVM = {Computational Visual Media})


% --------------------CITATIONS---------------------------

@inproceedings{shotton2013,
author = {Shotton, Jamie and Glocker, Ben and Zach, Christopher and Izadi, Shahram and Criminisi, Antonio and Fitzgibbon, Andrew},
title = {Scene Coordinate Regression Forests for Camera Relocalization in RGB-D Images},
booktitle = {Proc. Computer Vision and Pattern Recognition (CVPR)},
year = {2013},
month = {June},
abstract = {We address the problem of inferring the pose of an RGB-D camera relative to a known 3D scene, given only a single acquired image. Our approach employs a regression forest that is capable of inferring an estimate of each pixel’s correspondence to 3D points in the scene’s world coordinate frame. The forest uses only simple depth and RGB pixel comparison features, and does not require the computation of feature descriptors. The forest is trained to be capable of predicting correspondences at any pixel, so no interest point detectors are required. The camera pose is inferred using a robust optimization scheme. This starts with an initial set of hypothesized camera poses, constructed by applying the forest at a small fraction of image pixels. Preemptive RANSAC then iterates sampling more pixels at which to evaluate the forest, counting inliers, and refining the hypothesized poses. We evaluate on several varied scenes captured with an RGB-D camera and observe that the proposed technique achieves highly accurate relocalization and substantially out-performs two state of the art baselines.},
publisher = {IEEE},
url = {https://www.microsoft.com/en-us/research/publication/scene-coordinate-regression-forests-for-camera-relocalization-in-rgb-d-images/},
edition = {Proc. Computer Vision and Pattern Recognition (CVPR)},
}

@inproceedings{glocker2013,
author = {Glocker, Ben and Izadi, Shahram and Shotton, Jamie and Criminisi, Antonio},
title = {Real-Time RGB-D Camera Relocalization},
booktitle = {International Symposium on Mixed and Augmented Reality (ISMAR)},
year = {2013},
month = {October},
abstract = {We introduce an efficient camera relocalization approach which can be easily integrated into dense 3D reconstruction pipelines, such as KinectFusion. Our method is based on keyframes and makes use of randomized ferns which provide both compact encoding of frames and fast retrieval of pose proposals in case of tracking failure. During successful tracking, each frame/pose pair is considered as a potential keyframe. Only frames which are sufficiently dissimilar in the space of appearance are added to the set of keyframes. This keeps the scene representation compact and at the same time the coverage sufficiently dense. Frame dissimilarity is defined via the block-wise hamming distance (BlockHD) between the codes generated by the ferns. Distances between incoming frames and keyframes are efficiently and simultaneously evaluated by simply traversing the nodes of the ferns and counting co-occurrences with keyframes having equal sub-codes. For tracking recovery, camera pose proposals are retrieved from keyframes with smallest BlockHDs which are then used to reinitialize the tracking algorithm.

Both, online determination of keyframes and camera pose recovery are computationally efficient and have minimal impact on the run-time of the 3D reconstruction. Incorporating our method allows seamless continuation of reconstructions even when tracking is frequently lost. Additionally, we demonstrate how marker-free augmented reality can be realized by mesh-to-volume registration between an offline model and the online reconstruction. Our relocalization method is particularly appealing in such AR applications where the experience of steady pose tracking is essential.},
publisher = {IEEE},
url = {https://www.microsoft.com/en-us/research/publication/real-time-rgb-d-camera-relocalization/},
edition = {International Symposium on Mixed and Augmented Reality (ISMAR)},
}

@ARTICLE{Comaniciu2002,
  author={Comaniciu, D. and Meer, P.},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Mean shift: a robust approach toward feature space analysis}, 
  year={2002},
  volume={24},
  number={5},
  pages={603-619},
  doi={10.1109/34.1000236}}

@Inbook{Criminisi2013,
author="Criminisi, A.
and Shotton, J.",
editor="Criminisi, A.
and Shotton, J.",
title="Introduction: The Abstract Forest Model",
bookTitle="Decision Forests for Computer Vision and Medical Image Analysis",
year="2013",
publisher="Springer London",
address="London",
pages="7--23",
abstract="Problems related to the automatic or semi-automatic analysis of complex data such as photographs, videos, medical scans, text or genomic data can all be categorized into a relatively small set of prototypical machine learning tasks. The popularity of decision forests is mostly due to their recent success in classification tasks. However, forests are a more general tool which can be applied to many additional problems. This chapter presents a unified model of decision forests which can be used to tackle all the common learning tasks: classification, regression, density estimation, manifold learning, semi-supervised learning, and active learning.",
isbn="978-1-4471-4929-3",
doi="10.1007/978-1-4471-4929-3_3",
url="https://doi.org/10.1007/978-1-4471-4929-3_3"
}

@InProceedings{Klein2008,
author="Klein, Georg
and Murray, David",
editor="Forsyth, David
and Torr, Philip
and Zisserman, Andrew",
title="Improving the Agility of Keyframe-Based SLAM",
booktitle="Computer Vision -- ECCV 2008",
year="2008",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="802--815",
abstract="The ability to localise a camera moving in a previously unknown environment is desirable for a wide range of applications. In computer vision this problem is studied as monocular SLAM. Recent years have seen improvements to the usability and scalability of monocular SLAM systems to the point that they may soon find uses outside of laboratory conditions. However, the robustness of these systems to rapid camera motions (we refer to this quality as agility) still lags behind that of tracking systems which use known object models. In this paper we attempt to remedy this. We present two approaches to improving the agility of a keyframe-based SLAM system: Firstly, we add edge features to the map and exploit their resilience to motion blur to improve tracking under fast motion. Secondly, we implement a very simple inter-frame rotation estimator to aid tracking when the camera is rapidly panning -- and demonstrate that this method also enables a trivially simple yet effective relocalisation method. Results show that a SLAM system combining points, edge features and motion initialisation allows highly agile tracking at a moderate increase in processing time.",
isbn="978-3-540-88688-4"
}

@InProceedings{Holzer2012,
author="Holzer, Stefan
and Shotton, Jamie
and Kohli, Pushmeet",
editor="Fitzgibbon, Andrew
and Lazebnik, Svetlana
and Perona, Pietro
and Sato, Yoichi
and Schmid, Cordelia",
title="Learning to Efficiently Detect Repeatable Interest Points in Depth Data",
booktitle="Computer Vision -- ECCV 2012",
year="2012",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="200--213",
abstract="Interest point (IP) detection is an important component of many computer vision methods. While there are a number of methods for detecting IPs in RGB images, modalities such as depth images and range scans have seen relatively little work. In this paper, we approach the IP detection problem from a machine learning viewpoint and formulate it as a regression problem. We learn a regression forest (RF) model that, given an image patch, tells us if there is an IP in the center of the patch. Our RF based method for IP detection allows an easy trade-off between speed and repeatability by adapting the depth and number of trees used for approximating the interest point response maps. The data used for training the RF model is obtained by running state-of-the-art IP detection methods on the depth images. We show further how the IP response map used for training the RF can be specifically designed to increase repeatability by employing 3D models of scenes generated by reconstruction systems such as KinectFusion [1]. Our experiments demonstrate that the use of such data leads to considerably improved IP detection.",
isbn="978-3-642-33718-5"
}
