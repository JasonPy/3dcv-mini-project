% This version of CVPR template is provided by Ming-Ming Cheng.
% Please leave an issue if you found a bug:
% https://github.com/MCG-NKU/CVPR_Template.

%\documentclass[review]{cvpr}
\documentclass[final]{cvpr}

\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,colorlinks,bookmarks=false]{hyperref}


\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\confYear{CVPR 2022}
%\setcounter{page}{4321} % For final version only


\begin{document}

%%%%%%%%% TITLE
\title{Camera Pose Estimation using Regression Forest and RANSAC Optimization}

\author{Falk Ebert\\
\tt 4018276\\
{\tt\small falk.ebert@stud.uni-heidelberg.de}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Jason Pyanowski\\
\tt 3663907\\
{\tt\small jason.pyanowski@stud.uni-heidelberg.de}
\and
Marven Hinze\\
\tt 3664283\\
{\tt\small marven.hinze@stud.uni-heidelberg.de}
\and
Nadine Theisen\\
\tt 3475402\\
{\tt\small nadine.theissen@stud.uni-heidelberg.de}
}

\maketitle


%%%%%%%%% ABSTRACT
\begin{abstract}
   
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
cite it~\cite{shotton2013}

In this project, we provide an approach of interferring the pose of a camera in the world coordinate system. Given a data set consisting of RGB-D images as well as camera pose matrices, the camera for a given frame can be localized. The method we use is based on the paper by \cite{shotton2013}.  The whole process is divided into two subsequent steps which will be explained in more detail in the following sections.
\begin{enumerate}
\item Finding correspondencies between 2D image pixels and 3D world coordinates  \\

In the first step, we apply a \textit{Regression Forest} to a perform scene coordinate regression. A regression tree can be evaluated at any 2D image pixel \textbf{p} and predicts the corresponding position in the 3D world space of the scene as a combination of the predictions of all decision trees in the forest. We use the scene coordinates introduced by \cite{shotton2013} as labels for training the regression forest. Using the calibrated depth \textit{D}(\textbf{p}), it is possible to compute the 3D camera space coordinate \textbf{x} which can be transformed into the scene's world coordinate frame with the camera pose matrices \textit{H} giving  the scene world coordinates \textbf{m}. The forest is then trained on randomly chosen sets (\textbf{p,m}) for each tree.

\item Optimizing the camera pose for a given image \\
In this part, the scene coordinates associated with a 2D pixel will be used to estimate the camera location and orientation. The location is estimated by minimizing an energy function over a camera pose hypothesis \textit{H} where the number of pixels which are outliers according to the included error function is counted. The energy is optimized using a RANSAC algorithm. In the first step, the energy of a set of initial hypotheses is updated after evaluating the forest at a random sample of pixels.
\end{enumerate}
    
    

% What are we doing in this project?
%	-> Scene coordinate regression
% 	-> Camera pose estimation
% Why is this important?

% What data is given
% 	-> Image data including depth, camera poses
We use the 7-scenes data set which consists of seven different scenes with each between 2,000 and 12,000 frames. To each frame the corresponding depth map and the 4×4 matrix \textit{H} in homogeneous coordinates which encodes the rotation and the translation from camera space to world space. The full data set was obtained using a Kinect RGB-D camera at 640×480 resolution. The dataset is split into training and test data.

%-------------------------------------------------------------------------
\subsection{Related Work}


\section{Methods}
%-------------------------------------------------------------------------
This section gives an overview of the methods used in this project. We will discuss
the concept of regression forests including feature extraction and the RANSAC algorithm
used for estimating the camera pose from the image data. We will not discuss all aspects
in full detail but only provide further explanations where we find it relevant
for the presentation of our work and results.

\subsection{Data Preperation}
%-------------------------------------------------------------------------
To train the regression forest the 7-scenes datset~\cite{glocker2013} is utilized. 
Each scene consists of multiple image sequences that cover the are of interest. Using a RGB-D 
Kinect camera, depth information for each pixel is extracted. The resolution of the $24$ 
bit RGB-images is $640\times480$ pixels and the corresponding $16$ bit depth map is given 
in millimeters. For each image a $4\times4$ ground truth camera pose in homogenous coordinates 
is provided, which allows to validate the estimated camera pose matrix for a given image. 
Each dataset is split randomly into train and test. 

Based on that, our data set is composed of a number of samples $\{(\boldsymbol{p}_i, \boldsymbol{w}_i)\}$ with
$\boldsymbol{p}_i = (x_l, y_l)_i$ and $\boldsymbol{w}_i = (x_s, y_s, z_s)_i$ where the subscripts
$l$ and $s$ denote image pixel coordinates and scene coordinates respectively.\\

\subsection{Regression Forest}
%-------------------------------------------------------------------------

In this project we use a regression forest to predict scene coordinates for a given
sample image coordinate as suggested in \cite{shotton2013}. In this section we will
give some background on the concept of regression forests, the pixel coordinate
labeling and the image-feature extraction on which the forests base their predictions.\\

\subsubsection{Decision Trees}
%-------------------------------------------------------------------------
A regression forest consists of a number $N$ of regression trees, which each consist of
of root node and it's children. We consider the special case of binary decision trees
where each node (unless it is a leaf node) has a left and a right child. Each node $n$
stores a set of parameters $\theta_n$ which are used in the calculation of a
feature response function $f_{\theta}(\boldsymbol{p}) \in \{1, 0\}$ which determines if the input
data point $\boldsymbol{p}$ is branched to the left or right child node. To every leaf node
(i.e. a node without children) we assign a response value $\boldsymbol{w}_n$ representing
the tree's prediction for a data point $\boldsymbol{p}$ that has reached this node. In our
specific application the input data points $\boldsymbol{p}$ are 2D pixel coordinates and the
responses $\boldsymbol{w}$ are 3D world coordinates.

The process of training the tree involves finding the set of parameters
$\{\theta_n | \, \forall \, \text{nodes} \, n\}$ which results in the best predictions
for previously unseen input data points $\boldsymbol{p}$. However, this final goal cannot
be optimized directly during training, as this would entail optimization in the
very large space of all possible parameters for all tree-nodes simultaneously. Therefore
it is much more efficient to optimize the parameters one node at a time using a
proxy-objective $Q(S_{\text{tot}},\, S_{\text{left}}(\theta),\, S_{\text{right}}(\theta))$
which ideally leads to a similar result.

Let $S_{\text{tot},n} = \{ (\boldsymbol{p}_i, \boldsymbol{w}_i) \, | \, i \in |S_n|\}$ denote the
input data and target response for a given node $n$. According to its parameters the
node splits this set into the subsets
\begin{equation}
\begin{split}
	S_{\text{left},n}(\theta_n) = \{(\boldsymbol{p}_i, \boldsymbol{w}_i) \,|\, f_{\theta_n}(\boldsymbol{p}_i) = 0\} \\
	S_{\text{right},n}(\theta_n) = \{(\boldsymbol{p}_i, \boldsymbol{w}_i) \,|\, f_{\theta_n}(\boldsymbol{p}_i) = 1\}
\end{split}
\end{equation}
corresponding to left and right child node respectively. The objective function $Q$ is
then used to assign a score to the split resulting from the parameters $\theta_n$ at
this node.

The objective function used here optimizes for a reduction in variance of the target 
responses (i.e. "wants the tree to group together points with similar scene coordinates").
Defining $W_n = \{ \boldsymbol{w}_i \, | \, (\boldsymbol{p}_i, \boldsymbol{w}_i) \in S_n \}$ this
objective can be mathematically expressed as:
\begin{multline}
	Q(W_\text{tot}, \theta) =
		\text{Var}(W_\text{tot})\, - \sum_{d\,\in\{\text{left,\,right}\}}
			\dfrac{|W_d(\theta)|}{|W_\text{tot}|} \text{Var}(W_d(\theta))\\
	\text{with} \hspace{5mm} \text{Var}(W) = |W|^{-1} \sum_{\boldsymbol{w}\in W} ||\boldsymbol{w} - \bar{\boldsymbol{w}}||_2^2
\end{multline}
Training the complete tree then involves choosing a set of images from which a number
of sample pixels is drawn. These samples are then evaluated (i.e. split into left and right set)
by the tree's root node for a number of parameter-sets. For each set of parameters, the
split's score is calulacted using the objective function and the optimal parameters for
this node are recorded. The process is then repeated for the node's children with the corresponding
set (left or right; split according to the optimal parameters) until either the set contains
only one element or the maximum depth is reached.
When the maximum depth is reached and the set still contains more than one element, a response
needs to be calculated for the node. This response represents the tree's prediction for 
all pixels, which reach this node and is calculated from the sample's ground truth scene coordinates.
In practice, a mean shift variance algorithm \cite{Comaniciu2002} is used to find the center of the 
largest cluster. Thereby, at most $500$ random coordinates are utilized for calculation to limit 
the computational cost.

\subsubsection{Image Features}
%-------------------------------------------------------------------------
In order to use a decision tree to infer scene coordinates from the pixel coordinates,
it is necessary to calculate features associated with a given pixel coordinate from the
image data. This is implemented in the feature response function mentioned earlier.
We have chosen to use the 'Depth-Adaptive RGB' image features from \cite{shotton2013} which
is defined as
\begin{equation}
	f_{\theta}^{da-rgb}(\boldsymbol{p}) = I\left(\boldsymbol{p} + \frac{\boldsymbol{\delta}_1}{D(\boldsymbol{p})}, c_1\right)
	- I\left(\boldsymbol{p} + \frac{\boldsymbol{\delta}_2}{D(\boldsymbol{p})}, c_2\right)
\end{equation}
where $I(\boldsymbol{p}, c)$ is the images value in the $c \in \{\text{r, g, b}\}$ component at
the pixel location $\boldsymbol{p}$ and the 'Depth' image feature specified as
\begin{equation}
	f_{\theta}^{depth}(\boldsymbol{p}) = D\left(\boldsymbol{p} + \frac{\boldsymbol{\delta}_1}{D(\boldsymbol{p})}\right)
	- D\left(\boldsymbol{p} + \frac{\boldsymbol{\delta}_2}{D(\boldsymbol{p})}\right)
\end{equation}
where $D(\boldsymbol{p})$ refers to the depth value corresponding to pixel $\boldsymbol{p}$.
The parameters are given explicitly as $\theta = \{\boldsymbol{\delta}_{1,2}, c_{1,2}, \tau\}$. 
The offsets $\boldsymbol{\delta}_{1,2}$
allow the tree node to incorporate non-local information for each pixel. When the resulting
lookup location is outside the image boundary, the original sample pixel $\boldsymbol{p}$ is
disregarded in training and can also not be predicted by the forest. The parameter threshold 
$\tau$ is used to decide if the feature is split to the right or left child node according to 
$f_{\theta}(\boldsymbol{p}) \leq \tau$.


%-------------------------------------------------------------------------
\subsection{RANSAC Optimization}

%-------------------------------------------------------------------------
\section{Experiment Evaluation}


\subsection{Metrics} \label{subsec:metrics}
To evaluate the estimated camera poses, the translational and angular error with respect to the 
ground truth camera poses is measured. In particular, a translational error of at 
most $5$cm and angular error of at most $5^{\circ}$ is considered to be a correctly predicted pose. 

Let the estimated pose matrix $H_{est}$ and the ground truth $H_{gt}$ be $4 \times 4$ matrices
in homogeneous coordinates that encode the location and orientation of the camera in the world 
coordinate system. To measure the translational offset between them, the translation vectors 
$\boldsymbol{t_{est}}, \boldsymbol{t_{gt}} \in \mathbb{R}^3$ of the matrices are used.
By applying the $\ell^2$-norm to the translation vectors, the translational error is obtained by
\begin{align}
    \varepsilon_t &= \sqrt{\sum_{x,y,z}||\boldsymbol{t_{est}} - \boldsymbol{t_{est}}||^2}.
\end{align}
To compare two rotational poses, the angle of the difference rotation matrix $R$ is utilized. 
Denote the whole $3\times3$ upper-left matrix of $H_{est}$ and $H_{gt}$ as $R_{est}$ and 
$R_{gt}$ repectively. They refer to the orientation in space and the difference rotation can be computed 
as $R = R_{est}^TR_{gt}$. The angle between those two rotation matrices is the desired
error measure and defined as
\begin{align}
    \varepsilon_r &= arccos \left( \frac{tr(R)-1}{2} \right)
\end{align}
where $tr(R)$ is the trace of a matrix.

% \subsection{??}
% das vllt in die einleitung und ein chapter für die metrics vom tree 

%-------------------------------------------------------------------------
\section{Results}
In this section we present and evaluate our findings. For each scene a forest is trained with the hyperparameters given in 
Table~\ref{tab:params} and the 3D world coordinates are predicted by using unseen images from the test data set. To 
validate the results, the predicted coordinates of the forest are compared to the known ground truths using the $\ell^2$-norm and
is shown in the following subsection.
\begin{table}[h!]
	\begin{center}
	\begin{tabular}{|l|c|}
	\hline
	Hyperparameter & Value \\
	\hline\hline
	Number of Trees & $5$ \\
	Max Tree Depth & $16$ \\
	Batch Size & $500$ \\
	Pixel Samples per Image & $5000$ \\
	Parameter Samples per Image  & $1024$ \\
	Feature Type & Depth / DA-RGB \\
	\hline
	\end{tabular}
	\end{center}
	\label{tab:params}
	\caption{Hyperparameters used for training forests.}
\end{table}

Based on the predicted 2D-3D correspondencies of the forest, the camera pose is estimated by the RANSAC optimization.
For each image in the test data set, the respective camera pose is obtained and evaluated in terms of the translational 
and angular error with respect to the true camera poses. The findings for each scene are subsequently compared
to the results of~\cite{shotton2013}. Thereby, the amount of correctly classified camera poses is taken as a measure 
and is explained in Subsection~\ref{subsec:metrics}.


%At first, each trained forest (per scene) is analyzed in 
%terms of the ability to find 2D-3D correspondencies. Then, the estimated camera poses are evaluated as described 
%in~\ref{subsec:labels} and compared to the findings in~\cite{shotton2013}.


\subsection{Image to World Point Correspondencies}
Predicting the 3D world coordinates using 'Depth Adaptive RGB' features on the test data set, the forests exhibit 
$91.2\%$ valid coordinates on average. This implies that a forest is capable of finding correspondencies between 
image and world coordinates. However, the average deviations shown in Table~\ref{tab:forest-error} indicate rather 
large deviations from the ground truth. % aber nicht schlimm, weil ransac ja optimiert (?), earum aber so groß? vgl mit Paper

% table deviations
\begin{table}[h]
	\begin{center}
	\begin{tabular}{|l|c|}
	\hline
	Scene & average deviation [m]\\
	\hline\hline
	Chess 		& 	$1.060 \pm 0.4357$ \\
	Fire 		& 	$0.0 \pm 1.34$	\\
	Heads 		& 	$0.0 \pm 1.34$	\\
	Office 		&   $2.007 \pm 1.456$ \\
	Pumpkin 	& 	$1.477 \pm 0.7523$ \\
	RedKitchen 	& 	$1.747 \pm 1.017$ \\
	Stairs 		& 	$1.504 \pm 0.873$ \\
	\hline
	\end{tabular}
	\end{center}
	\label{tab:forest-error}
	\caption{Average deviation in meters between the predicted 3D world cordinates of the forest trained with 
	'Depth Adaptive RGB' features and the respective ground truths for each scene using the $\ell^2$-norm.}
\end{table}



\subsection{Camera Pose Predictions}
tranlational, angular error
plots

\section{Conclusion}

{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
